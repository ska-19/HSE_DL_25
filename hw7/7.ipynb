{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9915023e",
   "metadata": {},
   "source": [
    "# Обучение мультимодальной модель (Image -> Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84362186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.3\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b7c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.vocab = self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        for item in self.dataset:\n",
    "            tokens = self._tokenize_text(item[\"text\"])\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def _tokenize_text(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        if isinstance(item[\"image\"], bytes):\n",
    "            image = Image.open(io.BytesIO(item[\"image\"])).convert(\"RGB\")\n",
    "        elif isinstance(item[\"image\"], str):\n",
    "            image = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "        else:\n",
    "            image = item[\"image\"]  # Если уже PIL.Image\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = item[\"text\"]\n",
    "        tokens = [self.vocab[\"<SOS>\"]] + \\\n",
    "                [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in self._tokenize_text(text)] + \\\n",
    "                [self.vocab[\"<EOS>\"]]\n",
    "\n",
    "        return image, torch.tensor(tokens)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    captions = [item[1] for item in batch]\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    max_len = max(len(cap) for cap in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "\n",
    "    return images, padded_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a615fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mirandal/image_descriptions_cleaned\", split=\"train\")\n",
    "full_dataset = LogoDataset(dataset, transform=transform)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6681b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x).squeeze(-1).squeeze(-1)  # [batch, 256]\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        if hidden is None:\n",
    "            batch_size = x.size(0)\n",
    "            h0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden)  # output: [batch, seq_len, hidden_size]\n",
    "        output = self.fc(output)  # [batch, seq_len, vocab_size]\n",
    "        return output, hidden\n",
    "\n",
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(vocab_size, HIDDEN_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        images: [batch, 3, 224, 224]\n",
    "        captions: [batch, max_len]\n",
    "        \"\"\"\n",
    "        features = self.encoder(images)  # [batch, 256]\n",
    "        features = features.unsqueeze(1)  # [batch, 1, 256]\n",
    "        outputs, _ = self.decoder(captions[:, :-1])  # outputs: [batch, seq_len-1, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "model = ImageCaptionModel(vocab_size=len(full_dataset.vocab)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02aa371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.4933\n",
      "Epoch 2, Loss: 5.1822\n",
      "Epoch 3, Loss: 4.7457\n",
      "Epoch 4, Loss: 4.4049\n",
      "Epoch 5, Loss: 4.1224\n",
      "Epoch 6, Loss: 3.8853\n",
      "Epoch 7, Loss: 3.6724\n",
      "Epoch 8, Loss: 3.4795\n",
      "Epoch 9, Loss: 3.2974\n",
      "Epoch 10, Loss: 3.1284\n",
      "Epoch 11, Loss: 2.9698\n",
      "Epoch 12, Loss: 2.8177\n",
      "Epoch 13, Loss: 2.6759\n",
      "Epoch 14, Loss: 2.5340\n",
      "Epoch 15, Loss: 2.3957\n",
      "Epoch 16, Loss: 2.2671\n",
      "Epoch 17, Loss: 2.1433\n",
      "Epoch 18, Loss: 2.0243\n",
      "Epoch 19, Loss: 1.9155\n",
      "Epoch 20, Loss: 1.8049\n",
      "Epoch 21, Loss: 1.7039\n",
      "Epoch 22, Loss: 1.6106\n",
      "Epoch 23, Loss: 1.5285\n",
      "Epoch 24, Loss: 1.4444\n",
      "Epoch 25, Loss: 1.3622\n",
      "Epoch 26, Loss: 1.2822\n",
      "Epoch 27, Loss: 1.2067\n",
      "Epoch 28, Loss: 1.1381\n",
      "Epoch 29, Loss: 1.0758\n",
      "Epoch 30, Loss: 1.0130\n",
      "Epoch 31, Loss: 0.9535\n",
      "Epoch 32, Loss: 0.8985\n",
      "Epoch 33, Loss: 0.8494\n",
      "Epoch 34, Loss: 0.8015\n",
      "Epoch 35, Loss: 0.7575\n",
      "Epoch 36, Loss: 0.7155\n",
      "Epoch 37, Loss: 0.6744\n",
      "Epoch 38, Loss: 0.6358\n",
      "Epoch 39, Loss: 0.5987\n",
      "Epoch 40, Loss: 0.5651\n",
      "Epoch 41, Loss: 0.5348\n",
      "Epoch 42, Loss: 0.5070\n",
      "Epoch 43, Loss: 0.4799\n",
      "Epoch 44, Loss: 0.4546\n",
      "Epoch 45, Loss: 0.4355\n",
      "Epoch 46, Loss: 0.4163\n",
      "Epoch 47, Loss: 0.3972\n",
      "Epoch 48, Loss: 0.3796\n",
      "Epoch 49, Loss: 0.3617\n",
      "Epoch 50, Loss: 0.3468\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, captions in train_loader:\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, captions)\n",
    "\n",
    "        loss = criterion(\n",
    "            outputs.view(-1, outputs.size(-1)),\n",
    "            captions[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b68ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image, vocab, max_length=20):\n",
    "    model.eval()\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    features = model.encoder(image)\n",
    "    features = features.unsqueeze(1)\n",
    "    hidden = None\n",
    "\n",
    "    input_word = torch.tensor([vocab[\"<SOS>\"]]).to(device)\n",
    "    caption = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model.decoder(input_word.unsqueeze(0), hidden)\n",
    "        predicted_word = output.argmax(-1).item()\n",
    "\n",
    "        if predicted_word == vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        caption.append(predicted_word)\n",
    "        input_word = torch.tensor([predicted_word]).to(device)\n",
    "\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    return \" \".join([idx_to_word.get(idx, \"<UNK>\") for idx in caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f4d67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANwAAADcCAMAAAAshD+zAAAAclBMVEX///8PLWmGlbMfO3OKmLaos8nCytnv8fX5+vsjP3UaNnB3iKr29/pfc5vy9Pff4+vP1eFPZZG4wdM/V4dGXYt/j6+PnbkwSn4pRHmcqMHb3+hvgaWxu886UoSVor3e4urM0t9qfKJUaZRhdZyirsW9xdWCTKjlAAAPu0lEQVR4nO1dabuyrBaOSn1ySC2nSttq9f//4mEBFmgDYFjvubo/td0OLFisGZjNfvjhhx9++OGHH3744YcfftDF+VD9fboNplAjlH+6DW+GNQ829NccIe/l7evaNtygd+KE0CIkv7YIXfj/OHbUvzkMEB7e80RNGw+/QiiBHw5CLv+PC6bDtYR7wwIRFEInfB8uafdr7yIEf1gIBdwNoUvoEMg4YHLTOYzeF/Omv8PDVXd/lbixeNo1CPH85tFBSvjnFggBB0erjpO/D6FHBuUmPDwy7VYIOdxdCSVuxV3CjFuQHxe0+E7GvLSk0S3fOkxIEiG05e8LhiPnXPkxuw5c1vA98lGsdzlpsidyVYhHDf9jzl+7UOIEyZj0L8ycBXLnvqnmKuFMhQRCZe8fNrkqSv5M5N3rbfGeu3KBN652ZpqrBmhcZaU9MQHIetMLEC2PfUVXLvpd4xzgSt7vrU8gT7BYDAeDhNGKiuARKC2CNNmT2ZnU72qjNujkx0bJqf8fbFhWUq+wV53M5B4FBN+hHaA1/Zb4+NqLx9h4gXav+etYzGyJ6WINnzEPf96IFxJqkwjI7/Aqj3XQSdOW6PwrNsDl/tIddtgU8Iu+eMQ2yaIvv4O+lO8jYW9Z49fxAhMbO9B3jjfor0mAxeNCVLW4fX3xvXzl8IAyjDf2phK1/fkOj08K3J5WuHAc+qV7K3qhjPfMJxDMZmCLzwxZh6g/2x1XnDdycBpqbvJPYgXpftgAGzAmtpVjjfeEu/lR6BRsfqFsVNPG41/eY0xQ5PtHdysA91rxcdOy7pu8wVumSjiwpCfC2mqTGwvhLnZ5oWYn6nNuiEbWrnkzUpj7x+uffq43y54iQjpyaTyIgct7aAPGfAN2d/yLK8zJUCyhF5moW70eY74D0WNnwKkOb/5Yhz9sRfR7DhjzcT+/G2FuTLkfb1bwn71mv4Axp/KZ/4g1szTy7oZFssLDihMrHsqfGv5jsF/ynLLHn3VN9WVK7IbwwGxBJld8c6GcP5eXxRG2WxbR8U2GQh8QLElooKtyDX1DwIb3dWsXaCMSu33yjDZYvBjFIRZbvaCdCay37pUFL4y22RoPoBFeITp8SzRseSdkYhAwiIxVkhfevTYc22bMaMvk3d4GC93inYUYoTeBw5Q2EjjmW8yXp/VsPYHleZ7SuAUJ2folYiQa7VW/Pgb3QrCmAHYCxHZ3TJ7JxHm14Q6TGWaRIRpFtJNhpmEkzrFbBPwwgX2ST5pKu34ssu13ChNnS3mBs3mO8+wT/pYBxF3k7fvoiZKRNhLEIuuZE/cypF8A/17STAl7xPITWHOun986MTZFb7Koo+rcp4AZPwcp7szmz5CdL2Ntbocl18dEAdJrUBkL4T/b3sVy5vICvYKbeKU+KxxBGy3O3iidRyLd2OYhESkGGTf/NXEEJz07ICIC/OSQ1NCI1PIGdFpRA3sySM1hSeJwC9XZyqeOF5Fv2FdYjRAFzgnehLmxSACB3AyWJk493lNCdy+2bLoEo4aODZ6i2y0QF3OypEncPnWtSteHpJincUKX5mCc7UgL1yEv9FS8X4G4XrRqnxUidZUCa0KRTg6JvOXbfGVSKIJ1uTSeEQfvE8lbKfT+CTEjOu8VCOhDdfBeEHeLxlAoVOtB6q6GH/bifW6JBXNFOiH4kjhSsXiDghl19ZLHeAebTHwaT+X8fSOHe16QLAp51JhPM2kiGZQTnGvphyWIo/GsDgpmFBaU/Xy5rWjN7Ue5ATLEzU78TQpx1gyJ8j/KVZ1zj8YRHD0vToo4R5h2tfzbe2obv2eh2jwXlOsRFTpaUoq42YG/a4QF3Cp60hZCJMuXM/5WFE1yxIXC0OnbiKVi18SUTy5MZVropG9+Pc6nbfnbRoQxXKUYNDjg1dEBu5RwZTLGtnxMXMbfph+xiyqlQCP76omp171qtEKSOJu/TVM4h0uSVFNpX31gOjaA8T6olnRKErcWJp3SFyicXdI9reQYrs9sRjT22lWXtfeI+4eeQtWe8i26mAG5h4N6pjxiw7dSdikfjNxz4tR0zqVhvBVbRNWpL9Lzu+FTjAZojZxSgJ6xY76kLWv1HKC9t1BPQpsnbo4fWBzq7s+S6WV1WMrlalrEKbHlnrDjDXkzWbhfUlrOcv4+Ndb/XCGtLHGCTzdJ+WiW/Rv7CkniBL9gmix0QONLYyBJnOCvTlMbAdHlkUtoJIkTwkTTCIQl+daoii5J4nh5MlhMYghk3fao1WtyxAkRsOnWSNgk8RFrZ9HkiKv4u8aL9rV0c3fQPjfV5BUp4gRvbkx17799mTXJSsG8dMhsX+mtXpMhbs/fs9AbuPBy9OLbzFVQYXtSyKBVMSBBnC8wpdb8Ft4AUGoqrUfS2M1EgriYv0WvOvQ2ZNvUijL5LgrLtKWJOR2z6CVxfsvfkesxJe6fFlNl02j8Rioovy/TmGudTq++Iq4WLOZC02LwqCfhUHG075JbT5BxxmweZLaWvHxOnCNEY1Ghu4vPkTHigoaHJKJEjF9W7XKjr3qeEVc3Amko17b0LuzdW7rSPh8suB+gQG6SliNXsNwnzi+PaZCLpEHFhS5CxognagIE25fK8vyOGtH7xO3RAMUoc5kxor2ZdO2uJHGLbJy1vC2MrDB4ATni3PoDTRsPWbbcfsW2IIq4T1y4LQbkFf898h6rAmdvz4XMFRpbAzQ5Xlgo4S1/cf+Gr4ZEHQrPoYmmKK+P1qb+U1CUl8tmU5aWdT7vdscsW2qttJdxefjo0ErPApt3j+dJ23jZrnzxGrHEACkXClBIeeIX/itaE8/rtfUVg09InBC21KKuPLRJzketX4heP47btg2CU9McDp6XpqbYEmDxnKltQsE+jKW1y9qJtvSRzRWk3G26awT2LtuHcT5RYFeWOCEsq6cRYJsi6up4b6vAfA5p4vhpp7NGgGkU8ns70c4+0sTNeG2ubuI38FhABUk53B3ODOSJ4/UBqhU/A+GKogYBH6exNmOrQp44IfioOHRAFazoOLLHKyXv0LezQKVu+4oHxNkcOqNpzt+qFgDedGVHGfleIm+HRbuGdqpOza1Ewr9TSRF/US3VE1WrLmO5mS8lE6ah5d3WNmi5yxJFNld9y1vQpiWC3YprVbScSYWRE0tK6/EEPEOnefJmd1noFrCqjJyQgjS83cMa1ralEJD9h7Virle/qjJyQr2s6bWj18AlOBSaIQAV4j5QjEL5U9fKVmFLwUhR2oT731K36hu2Cn2ZNXkEpZFr+MsKHpYf6y7IhcSnmsbnoUScoMYV1o0F2pEz2K1d/4AIJbY885fli6TOnShfNwUqDgphIvCRWRLWt9VdQKWRE4qk5KMaFVoRznLYrkjS7g5syA+WDbZVtjoVtkrE1fxlaXEZduUdXXJd1uqGbdiqS9YlxdULYJTYUkwhyNrOF7aWAGyAuRPJ790jrI9ChXqVmxJxeguWSmrSw+6zINR3sjp5d/2SG8+1ksdKbOkLxMlOcHZYQMCEuiVLHN33IPFeBXAfQ2nkxIp8absBpg45fKOGvzzZlU7HqtmNS70ojZxof0mr5S7oSWxtx51ur0a1kVvx1+VNFBJdcIkvDdvtfLg6/dHICcQpOOPrzbwkmqNeTbl93AjidEqdnQVKRhedS2Ni4mYX3eSzs8l2qtXqUxOnCYuGwAq1vQJME9cu3+DUbiq1j3YwTFz5hrNfauojV14AmkglfmmYOOIHjDr7hZ6vgk5gppDNmhTCwWaJO5L1itj80t0SL2zIp9rOUomVGNMocdgaWfkWGb1Yz5Ai6p/bCTvqHy3zFEaJW1IbjZIXaNm/K1RtxL8V1uAbJa5CLpGVLOesEmDoUPbmq9JOrkaJY6EBzJ1xA0+46djVJOBRyt9tmDiSHiYb5EVEoNfyLbsLta2hjRKH6SnKc8IatBy/DSvEFhUE74N9v/irwW0zMFeNuGu0jMSME61VHRxsKCJU6R+FHdv6kFA4JNC5mJPAqjXSGo1INb7S2Jslbva3nB+dEKvytBl3gM8fMVQKNVvHMHEULEanv/0XNVSUrdRJiFsDR7n6BjSp+XPV/YtJiMPm784asQEksKT3NaeCzpxab2nRffyh5juOJQQcQWa73vsa9D2j9tcV+KnsqfgfARzCh6qELBP+zkOCRwAbIitQR8QVM1wQOzW7YqPEpZGAGgZv9DaJz+AUEx9auL0RRE4eMnVoFKCa9vQTkuW6KgGyD7/Bz0M6M57wbL9QyF2vIVMcm9vKHfYVHRz6ZQ7YVXb5v8Gs1C6SeY1oNWYprTLyXuIuRYWRrnWoEgXFo7/QQRWwSYUQr9oZOavKT1ySFoHE5oQrRnPz6m1GteiGVpUnxkWKde2+vWpEXw9Q2WpFrvaOCgoIuJkGB7SZP1qM7gQ3yYQ78MtETCvviLr0NNNu/PQ5eg737XQ9h2xrZepbWEDOu29eqz6deYXy09iNwh4Aoni3GMk6MWg77Chf+PCNLmDUnZjdvnUCOss2JZK/EdcvBQY5E6hrSVzmOs8rqPeGua5feTrEhYRpYQm501M4nkExVlJZcisWKeli2rp6a492i3NWB7KknA/eHQ2KMRrKvpl1h648qXrr4rW4TeNbkP3tR/A+AljMHJ803Sjab1VChPVqmGHbupjQEooWW252YQ95QXVCwaj03zcnKtJz9iSGF0PEtx5qaqk2v7DLJWp1Ajj28RDH4vkPFzZknvaeZ2MxsMNOOnsC2t3CCf6MpTkzJqEDjZ6E+BhgQfPnpS80Ep1c4s69mQNZF1s/TsmYDM6ZDFEpCGubXwTXyHldhLZ2c9mBg+1ex+7C5jPZb9O8QyCiYdL/wA+dxym8SG6ZAJTeb2kvgL1zO2YF8+oinQPHaq0tHYUNo6Pm13HknMe8lAt0BNxKNlhBdLV+nC6CfpgwFtVhS/kx4+b7no/kbKW29lsjfu1Vyq+h81Ni2H0kfA7eMS0Qvw5Wxu2xECK59R0FqSVwMkIhGJN80C6y67e1Vw02K7K4TTmYIG67I7phJ2lawDTdBN3w5xMevfscDvCNe7p29XVxx3Zew659Ujndbi0PJW470fYRUtjz/YytsbzbuRJGVc7iBZcQTiQhWE20fYQ6WqgX8jcH5sEOl9f+3Wt4yu3NL6k+PgG3mzDREmZff1cGOIYlHkYkeOtNra5zSmz4oszVQDJE1EV7JtmbCb03RRy4bq+H1adss6d7KzfPMdYm67JCIw+9NYgVNyzpMPDQrVEcJp/AMHFpJfzEWU15ROltVKoh/50eEtcFTWje++sR9jJqAHaA0r2kqB2A5tjuPmBEauB4z8EkA2QmsTYpztU9/zI8BsvvOrlbE87/BRU//PDDDz/88MMPP/zwww8//PBm/A/gdbSz1RHecgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 300
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the logo features a stylized letter 'g' in black set against a white background within a square border. the design\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "test_image_path = \"test_logo.png\"\n",
    "display(IPImage(filename=test_image_path, width=300))\n",
    "print(generate_caption(model, test_image_path, full_dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe44a92",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- Модель обучается и генерирует текст учитывая изоброжение логотипа, но хорошо бы обучить её на большем количестве изображений логотипов дав больше времени на обучение и вариации изображений.\n",
    "- Видно упоминание ключевых слов, связанных с логотипами. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
